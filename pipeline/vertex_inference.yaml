# Vertex AI Infrastructure Configuration for Inference
# This file contains infrastructure settings optimized for inference workloads
# Inference requires less resources than training (no gradients, no optimizer states)

# ============================================================================
# VERTEX AI SETTINGS - INFERENCE
# ============================================================================
vertex:
  # Project settings (same as training)
  project_id: "wmt-81b2d4df3ac412f206f24f3ddc"
  region: "us-central1"
  
  # Service account and networking (same as training)
  service_account: "1048186071185-compute@developer.gserviceaccount.com"
  network: "projects/12856960411/global/networks/vpcnet-private-svc-access-usc1"
  
  # Storage (same as training)
  staging_bucket: "gs://p13n-storage2/user/y0c07th/exp/cnn_set_pb"
  
  # Container (same as training)
  container_uri: "us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-4.py310:latest"
  
  # Package settings
  package_name: "src"
  package_version: "0.3.0"
  module_path: "inference"  # Points to src/inference.py (not runner.py)
  
  # Job settings - INFERENCE PROFILE
  job_name: "set-pb-inference"
  run_type: "inference"
  
  # ============================================================================
  # COMPUTE RESOURCES - OPTIMIZED FOR INFERENCE
  # ============================================================================
  # Inference needs:
  #   - 1 GPU is sufficient (no gradient computation, no multi-GPU needed)
  #   - Less CPU/RAM than training (no optimizer states to store)
  #   - Can use larger batch sizes (forward pass only)
  
  # RECOMMENDED: GPU-accelerated inference (fast and cost-effective)
  machine_type: "n1-highmem-16"      # 16 vCPU, 104 GB RAM
  accelerator_type: "NVIDIA_TESLA_T4"
  accelerator_count: 1                # 1 GPU is enough for inference
  replica_count: 1
  
  # ============================================================================
  # ALTERNATIVE CONFIGURATIONS (comment/uncomment as needed)
  # ============================================================================
  
  # Option A: Minimal GPU (cheaper, still fast)
  # machine_type: "n1-standard-8"      # 8 vCPU, 30 GB RAM
  # accelerator_type: "NVIDIA_TESLA_T4"
  # accelerator_count: 1
  # Cost: ~$1.50/hour | Speed: Good | Batch size: 64-128
  
  # Option B: Larger GPU memory (for very large batches)
  # machine_type: "n1-highmem-32"      # 32 vCPU, 208 GB RAM
  # accelerator_type: "NVIDIA_TESLA_T4"
  # accelerator_count: 1
  # Cost: ~$3.00/hour | Speed: Fast | Batch size: 256+
  
  # Option C: CPU-only (cheapest, slower)
  # machine_type: "n1-highmem-32"      # 32 vCPU, 208 GB RAM
  # accelerator_count: 0               # No GPU
  # Cost: ~$1.50/hour | Speed: Slow | Batch size: 64
  
  # Option D: Premium GPU (A100 for very fast inference)
  # machine_type: "a2-highgpu-1g"
  # accelerator_type: "NVIDIA_TESLA_A100"
  # accelerator_count: 1
  # Cost: ~$4.00/hour | Speed: Very fast | Batch size: 256+

# ============================================================================
# USAGE NOTES
# ============================================================================
# 
# To use this config for inference:
#   python pipeline/vertex_inference_job.py \
#     --config_file pipeline/vertex_inference.yaml \
#     --inference_config configs/inference.yaml \
#     --checkpoint gs://bucket/checkpoint.pt
# 
# Resource Comparison (Training vs Inference):
#   Training (vertex.yaml):
#     - Machine: n1-highmem-96 (96 vCPU, 624 GB RAM)
#     - GPUs: 4x NVIDIA_TESLA_T4
#     - Cost: ~$8-10/hour
#   
#   Inference (this file):
#     - Machine: n1-highmem-16 (16 vCPU, 104 GB RAM)
#     - GPUs: 1x NVIDIA_TESLA_T4
#     - Cost: ~$2.50/hour
#     - Savings: ~70% cheaper than training
# 
# Why inference needs less:
#   ✓ No backward pass (no gradients)
#   ✓ No optimizer states
#   ✓ Forward pass only
#   ✓ Can use larger batch sizes (more efficient)
#   ✓ 1 GPU is sufficient for most workloads

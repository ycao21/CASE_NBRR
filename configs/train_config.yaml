data:
  train_path: "../data/instacart_train"
  test_path: "../data/instacart_test"

  train_pattern: "*.parquet"
  test_pattern: "*.parquet"
  
  max_train_files: null
  max_test_files: null
  
  val_split_ratio: 0.05
  split_seed: 42
  
  # GCS settings
  download_from_gcs: true
  temp_dir: "/tmp/case_data"
  cleanup_temp_on_exit: true
  
  column_mapping:
    item_ids: "item_id_arr"
    labels: "label_arr"
    membership: "member_day_arr"

vocabulary:
  vocab_path: null              # Set to path if you have pre-built vocab
  build_from_data: true         # Build from training data
  save_vocab_path: "vocab/item_to_idx.json"
  
  vocab_size: null            
  num_categories: null         

sequence:
  max_sequence_length: 1024      
  truncate_strategy: "recent"   
  
  item_padding_idx: -1          
  label_padding_value: -100     
  membership_padding_value: 0.0 

model:
  vocab_size: null             
  num_categories: null         
  
  embedding_dim: 128           
  cnn_layer_enabled: true      
  cnn_output_dim: 128          
  hidden_dim: 256              
  dropout: 0.1
  membership_dim: 364          
  
  cnn_kernel_sizes:
    weekly: 7
    biweekly: 14
    monthly: 28,
    seasonal: 90,
    trend: 180
  
  set_phi_type: "st_encoder"   
  perm_eq_num_stacks: 2        
  num_heads: 4                 
  num_inds: 32                 
  category_pooling_type: "pma"  # "average" or "pma" (Pooling by Multihead Attention)
  pma_num_heads: 4              # Number of heads for PMA pooling
  
  scoring_mode: "both"          # "both", "intrinsic_only", "compatibility_only"

batch:
  batch_size: 16
  num_workers: 4
  prefetch_factor: 2
  shuffle_train: true
  shuffle_val: false
  pin_memory: true

training:
  num_epochs: 3  
  gradient_accumulation_steps: 4  
  max_grad_norm: 1.0
  
  log_interval: 100              
  val_interval: 1000             
  save_interval: 1000             
  debug_steps: 1000               
  
  k_values_train: [1, 5, 10, 20] 
  primary_metric: "Precision@10"    

optimizer:
  type: "adamw"
  learning_rate: 0.001
  weight_decay: 0.01
  betas: [0.9, 0.999]

scheduler:
  type: null                   
  step_size: 1000
  gamma: 0.9
  # Cosine scheduler
  t_max: 10000
  # Plateau scheduler
  patience: 3
  factor: 0.5

loss:
  margin: 1.0 
  neg_sample_ratio: 10          # Number of negative samples per positive
  sampling_strategy: "hard"     
  # Options: "random (random: 1.0) for strategy_weights ", 
  #          "hard (hard: 1.0)", 
  #          "frequency (frequency: 1.0)", 
  #          "recency (recency: 1.0)", 
  #          "mixed (hard: 0.4, frequency: 0.3, recency: 0.2, random: 0.1)"
  
  # Mixed sampling strategy weights (used when sampling_strategy="mixed")
  strategy_weights: # mixed
    hard: 0.4                   # 40% hard negatives
    frequency: 0.3              # 30% frequency-based
    recency: 0.2                # 20% recency-based
    random: 0.1                 # 10% random

debug:
  limit_samples: null          
  limit_eval_samples: null     
